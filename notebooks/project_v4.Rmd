---
title: "Statistics Project"
author: "Roberta"
date: "11/1/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(PerformanceAnalytics)
library(performance)
library(reshape2)
library(ggforce)
library(MASS)
library(car)
library(mgcv)
set.seed(123)
```

## Exploratory Data Analysis

The dataset of interest contains information about the Covid-19 spreading outbreak (on 30 variables) and is taken from the official website of Protezione Civile.

Download the csv data replacing blanks with NA.
Extract the data for Lombardia region and restrict the data range to the interval 1st October 2020 to 1st February 2021.
The number of observations left is 123.

Some of the 30 variables miss information for around 50% or more of the observations. These are:

* `totale_positivi_test_molecolare`
* `totale_positivi_test_antigenico_rapido`
* `codice_nuts_1`
* `codice_nuts_2`
* `tamponi_test_molecolare`
* `tamponi_test_antigenico_rapido`
* `casi_da_sospetto_diagnostico`
* `casi_da_screening`
* `ingressi_terapia_intensiva`

Some of the variables are completely missing (will be ignored):

* `note`
* `note_test`       
* `note_casi`

In particular, variables related with the tests are available only after date 2021-01-15 (will be ignored) :

* `tamponi_test_molecolare`
* `tamponi_test_antigenico_rapido`
* `totale_positivi_test_molecolare`
* `totale_positivi_test_antigenico_rapido`

This could be related to regional rules, maybe before that date it was not mandatory to provide the details of the tests performed per day and their outcome.

Assuming to exclude all the previously mentioned variables, and variables related to the region identification (such as stato, codice_regione, denominazione_regione, lat, lon) being the response variable the `totale_ospedalizzati`, the **variables left for modeling are 11**:

* `ricoverati_con_sintomi`: Hospitalised patients with symptoms.
* `terapia_intensiva`: Intensive Care.
* `isolamento_domiciliare`: Home confinement -> **Newly added by Valeria to df**.
* `totale_positivi`: Total amount of current positive cases (Hospitalised patients + Home confinement).
* `variazione_totale_positivi`: New amount of current positive cases (totale_positivi current day - totale_positivi previous day).
* `nuovi_positivi`: New amount of current positive cases (totale_casi current day - totale_casi previous day).
* `dimessi_guariti`: Recovered.
* `deceduti`: Death (cumulated values).
* `totale_casi`: Total amount of positive cases.
* `tamponi`: Tests performed.
* `casi_testati`: Total number of people tested.

ATT: **`ingressi_terapia_intensiva` resta fuori dalla lista** perch√© carente almeno al 50%, va bene?

5 variables out of the former list are cumulative so they are converted in **per_day** data:

* `dimessi_guariti`
* `deceduti`
* `totale_casi`
* `tamponi`
* `casi_testati`

```{r include=FALSE}
df<-read.csv("https://raw.githubusercontent.com/pcm-dpc/COVID-19/master/dati-regioni/dpc-covid19-ita-regioni.csv",na.strings=c("",NA))

beg<-"2020-09-25T00:00:00"
end<-"2021-02-18T00:00:00"

df_sub <- df[df$denominazione_regione == 'Lombardia',]
df_sub <- df_sub[df_sub$data >= beg & df_sub$data <=end,]
summary(df_sub)

df_sub <- df_sub[,c("data",
                    "totale_ospedalizzati",
                    "ricoverati_con_sintomi", 
                    "totale_positivi", 
                    "nuovi_positivi", 
                    "dimessi_guariti", 
                    "deceduti",
                    "variazione_totale_positivi", 
                    "terapia_intensiva",
                    "casi_testati", 
                    "totale_casi", 
                    "tamponi",
                    "isolamento_domiciliare"
)]



dimessi_guariti_per_day <- diff(df_sub$dimessi_guariti)
df_sub$dimessi_guariti_per_day <- c(dimessi_guariti_per_day[1],dimessi_guariti_per_day)

deceduti_per_day <- diff(df_sub$deceduti)
df_sub$deceduti_per_day <- c(deceduti_per_day[1],deceduti_per_day)

casi_testati_per_day <- diff(df_sub$casi_testati)
df_sub$casi_testati_per_day <- c(casi_testati_per_day[1],casi_testati_per_day)

totale_casi_per_day <- diff(df_sub$totale_casi)
df_sub$totale_casi_per_day <- c(totale_casi_per_day[1],totale_casi_per_day)

tamponi_per_day <- diff(df_sub$tamponi)
df_sub$tamponi_per_day <- c(tamponi_per_day[1],tamponi_per_day)


```

### Adding covariates: Lombardia (ITC4) region color in time -> check for legislation in that period!

Extracting zone colors for the analysis reference period.
Assuming that effects of restriction rules can be seen only after a certain 
amount of time shift the colors by 14 days forward.

```{r, echo=FALSE}
aree_zone <- read.csv("https://raw.githubusercontent.com/ondata/covid19italia/master/webservices/COVID-19Aree/processing/areeStorico_wide.csv")
aree_zone <- aree_zone[aree_zone["datasetIniISO"] >= "2020-09-25", ]
aree_zone <- aree_zone[aree_zone["datasetIniISO"] < "2021-02-18", ]
aree_zone <- data.frame(aree_zone$datasetIniISO, aree_zone$ITC4)

column_new_dates <- data.frame(seq(from = as.Date(beg), to = as.Date(end), by= "day"))
colnames(column_new_dates) <- "data"
colnames(aree_zone) <- c("data", "color")
aree_zone$data <- as.Date(aree_zone$data)

#Remove rows with empty color
aree_zone <- aree_zone[!(is.na(aree_zone$color) | aree_zone$color==""), ]

aree_zone_merged <- merge(column_new_dates, aree_zone, by = "data", all.x = TRUE)

aree_zone_merged[1,2] <- "bianca"

last_color = "bianca"
for (i in 1:nrow(aree_zone_merged)){
    if(is.na(aree_zone_merged[i,2])){
        aree_zone_merged[i,2] = last_color
    }else{
        last_color = aree_zone_merged[i,2]
    }
}

# Delay the colors by 2 weeks: shift forward the color vector by 14 cells
# prepending 14 cells (days) of zona bianca
d_color <- aree_zone_merged$color
color_delayed <- c(rep("bianca",times=14), d_color[1:(length(d_color)-14)])
aree_zone_merged_ext <- aree_zone_merged
str(aree_zone_merged_ext)
aree_zone_merged_ext$color_delayed <- as.factor(color_delayed)

df_sub$data <- as.Date(df_sub$data)

df_sub <- merge(df_sub, aree_zone_merged_ext, by = "data")
#df_sub
df_sub$color <- as.factor(df_sub$color)
df_sub$color_delayed <- as.factor(df_sub$color_delayed)
```


```{r}
##Colore grafico
```



## Adding covariates: Lombardia (ITC4) RT in time 

Rt for the reference interval

```{r, echo=FALSE}
rt_lomb <- read.csv("iss_rt_lombardia.csv")
rt_lomb <- rt_lomb[rt_lomb["data"] >= "2020-09-25", ]
rt_lomb <- rt_lomb[rt_lomb["data"] < "2021-02-18", ]


rt <- data.frame(data=as.Date(rt_lomb$data), rt=rt_lomb$rt_positivi)
summary(rt)

df_sub$data<-as.Date(df_sub$data)
df_sub <- merge(df_sub, rt, by='data')

```

```{r}

df_sub$dummy = 0
i=1

for(data in df_sub$data){
    if(data >= as.Date("2020-11-22")){
        df_sub[i, "dummy" ] = 1
    }
    i = i+1
}

df_sub_ext<-df_sub

```
df_sub_ext contiene i dati da 25/09 a 18/02
```{r}
inizio_train<-"2020-10-01T00:00:00"
end_train<-"2021-02-01T00:00:00"
df_sub <- df_sub[df_sub$data >= inizio_train & df_sub$data <=end_train,]
```

### Data quality

Plotting the variable of interest:

```{r , echo=FALSE}
ggplot(df_sub, aes(x=as.Date(data), y=totale_ospedalizzati)) +
  geom_line() +
  xlab("Date") +
  scale_y_continuous(breaks = seq(0,9500,by=1000))+
  geom_linerange(aes(ymax=totale_ospedalizzati, 
                     ymin=df_sub$totale_ospedalizzati[which.min(df_sub$totale_ospedalizzati)]), 
                 data=df_sub[which.max(df_sub$totale_ospedalizzati),], 
                 col="red", lty=2) + 
  geom_text(aes(label=totale_ospedalizzati), 
            col="red", data=df_sub[which.max(df_sub$totale_ospedalizzati),], 
            hjust=7, vjust=-0.4) + 
  geom_text(label=as.Date(df_sub[which.max(df_sub$totale_ospedalizzati),c("data")]), 
            col="red",
            x=as.Date(df_sub$data[which.max(df_sub$totale_ospedalizzati)]),
            y=100,
            size=3) +
  geom_segment(x = as.Date(df_sub$data[1]),
               y = df_sub$totale_ospedalizzati[which.max(df_sub$totale_ospedalizzati)], 
               xend = as.Date(df_sub$data[which.max(df_sub$totale_ospedalizzati)]), 
               yend = df_sub$totale_ospedalizzati[which.max(df_sub$totale_ospedalizzati)], 
               colour = "red",
               lty=2
               )
```

Plotting all the relevant columns in time

```{r echo=FALSE}
df.m <- melt(df_sub, "data")

ggplot(df.m, aes(as.Date(data), value, group = 1)) + 
  geom_line() + 
  facet_wrap(~variable, scales = "free") +
  xlab("Date")

```

OSS1: `totale_ospedalizzati` is exact the sum of:`ricoverati_con_sintomi` and `terapia_intensiva`.

OSS2: `totale_ospedalizzati` is the difference of: `totale_positivi` - `isolamento_domiciliare`.

Thus these 4 variables out of the list of 11 variables will be omitted. (**Think about a graphic plot to show it**).
Remaining variables are 7:

* `variazione_totale_positivi`: not interesting.
* `nuovi_positivi`: interesting.
* `dimessi_guariti_per_day`: interesting ?
* `deceduti_per_day`: interesting ?
* `totale_casi_per_day`: interesting ?
* `tamponi_per_day`: not interesting ?
* `casi_testati_per_day`: not interesting ?

OSS3: `nuovi_positivi` is equal to `totale_casi_per_day` -> `totale_casi_per_day` not interesting!

```{r , echo=FALSE}
ggplot(df_sub, aes(x=as.Date(data), y=nuovi_positivi, color="nuovi_positivi")) +
  geom_line() + 
  geom_line(data=df_sub,aes(x=as.Date(data),y=totale_casi_per_day, color="totale_casi_per_day")) +
  xlab("Date") + 
  ylab("Daily count") + 
  scale_color_manual(name="Variable", values = c(1:2))
  #scale_y_continuous(breaks = seq(0,9500,by=1000))+

```

First let's have a close look to all plus `totale_ospedalizzati`: (there are no NA)

```{r}
summary(df_sub$variazione_totale_positivi)
summary(df_sub$nuovi_positivi)
summary(df_sub$dimessi_guariti_per_day)
summary(df_sub$deceduti_per_day)
summary(df_sub$tamponi_per_day)
summary(df_sub$casi_testati_per_day)
summary(df_sub$totale_ospedalizzati)
```

Now let's look at the boxplot to spot outliers:

```{r , echo=FALSE, fig.width=6, fig.height=6}
# ggplot(df_sub, aes(y=nuovi_positivi)) + 
#   geom_boxplot()
varnames <- c("totale_ospedalizzati", "nuovi_positivi", "variazione_totale_positivi", 
              "dimessi_guariti_per_day", "deceduti_per_day", "casi_testati_per_day",
               "tamponi_per_day")
indexes <- c(2,5,8,14,15,16,18)
# boxplot(df_sub$nuovi_positivi,
#         xlab=varnames[2],
#         ylab="Count"
#         )
par(mfrow = c(3, 3))
invisible(lapply(indexes, function(i) boxplot(df_sub[, i], ylab="Count", main=colnames(df_sub)[i])))
#stripchart(df_sub$nuovi_positivi, method = "jitter", pch = 7, add = TRUE, col = "blue")
```

So the more affected by outliers are all except `tamponi_per_day` in order of gravity:

* `variazione_totale_positivi`: non ci interessa.
* `dimessi_guariti_per_day`: **GRAVE perch√© superano gli ospedalizzati** -> dati aggiornati con discontinuit√†.
* `nuovi_positivi`: non ci interessa troppo.
* `casi_testati_per_day`: **NON GRAVE** -< sono sempre molto meno degli ospedalizzati. 
* `deceduti_per_day`: non ci interessa troppo.


ATT: We will not consider `variazione_totale_positivi` anymore -> list of covariates interesting is now with 5 elements:

* `nuovi_positivi`: interesting.
* `dimessi_guariti_per_day`: interesting **BUT outliers**.
* `deceduti_per_day`: interesting.
* `tamponi_per_day`: not interesting ?
* `casi_testati_per_day`: not interesting ?

Scatterplot matrix of 3 interesting variables:

```{r , echo=FALSE}
chart.Correlation(df_sub[,c("nuovi_positivi", 
                            "dimessi_guariti_per_day",
                            "deceduti_per_day",
                            "totale_ospedalizzati"
                            )])

```

Another scatterplot with remaining 3 variables:

```{r , echo=FALSE}
chart.Correlation(df_sub[,c("casi_testati_per_day",                            
                            "tamponi_per_day", 
                            "totale_ospedalizzati"
                            )])

```

Plotting all 5 interesting variables for the response variable `totale_ospedalizzati`.

This is per date to see outliers.

```{r , echo=FALSE}
ggplot(df_sub, aes(x=as.Date(data), y=nuovi_positivi, color="nuovi_positivi")) +
  geom_line() + 
  geom_line(data=df_sub,aes(x=as.Date(data),y=totale_ospedalizzati, color="totale_ospedalizzati")) +
  geom_line(data=df_sub,aes(x=as.Date(data),y=dimessi_guariti_per_day, color="dimessi_guariti_per_day")) +
  geom_line(data=df_sub,aes(x=as.Date(data),y=deceduti_per_day, color="deceduti_per_day")) +
  geom_line(data=df_sub,aes(x=as.Date(data),y=casi_testati_per_day, color="casi_testati_per_day")) +
  geom_line(data=df_sub,aes(x=as.Date(data),y=tamponi_per_day, color="tamponi_per_day")) +
  xlab("Date") + 
  ylab("Daily count") + 
  scale_color_manual(name="Variable", values = c(1:6))
  #scale_y_continuous(breaks = seq(0,9500,by=1000))+

```





List of covariates interesting is now with 3 elements since `tamponi_per_day` and `casi_testati_per_day` do not seems intersting. -> **check later**

* `nuovi_positivi`: interesting.
* `dimessi_guariti_per_day`: interesting **BUT outliers**.
* `deceduti_per_day`: interesting.

Plotting all 3 interesting variables for the response variable `totale_ospedalizzati`.

```{r , echo=FALSE}
ggplot(df_sub, aes(x=as.Date(data), y=nuovi_positivi, color="nuovi_positivi")) +
  geom_line() + 
  geom_line(data=df_sub,aes(x=as.Date(data),y=totale_ospedalizzati, color="totale_ospedalizzati")) +
  geom_line(data=df_sub,aes(x=as.Date(data),y=dimessi_guariti_per_day, color="dimessi_guariti_per_day")) +
  geom_line(data=df_sub,aes(x=as.Date(data),y=deceduti_per_day, color="deceduti_per_day")) +
  xlab("Date") + 
  ylab("Daily count") + 
  scale_color_manual(name="Variable", values = c(1:4))
  #scale_y_continuous(breaks = seq(0,9500,by=1000))+

```


Calculate The day week

```{r , echo=FALSE}

df_sub <- add_column(df_sub, weekdays(df_sub$data), .after="data")
names(df_sub)[names(df_sub) == "weekdays(df_sub$data)"] <- "giorno_settimana"

distinct_days <- as.factor(df_sub$giorno_settimana)


count_week_day_dimessi <- tapply(df_sub$dimessi_guariti_per_day, df_sub$giorno_settimana, mean, simplify = FALSE)
count_week_day_positivi <- tapply(df_sub$nuovi_positivi, df_sub$giorno_settimana, mean, simplify = FALSE)


barplot(height = as.numeric(count_week_day_dimessi), names=levels(distinct_days))

barplot(height = as.numeric(count_week_day_positivi), names=levels(distinct_days))


```

From the previous slide, we have seen that there are some outliers that we must take into account, but they can have some problems when creating the models. So we create a "moving average" for those variables that have outliers. 

```{r, echo=FALSE}

compute_mean_points <- function(var, from, to){
    var_mean <- c()
    for (i in 1:length(var)){
        if(i >= length(var)-to){
            var_mean <- append(var_mean, mean(var[(i-from):(length(var)-((length(var))-i))])) #mean tra i valori finali
        }else if(i <= from){
            var_mean <- append(var_mean, mean(var[1:(i+to)])) #mean tra i valori iniziali
        }else{
            var_mean <- append(var_mean, mean(var[(i-from):(i+to)])) #mean nei valori di mezzo
        }

    }
    return(var_mean)
}

#Dismessi guariti per day
var_mean <- compute_mean_points(df_sub_ext$dimessi_guariti_per_day, 3, 3)
df_sub_ext$mean_dimessi_guariti_per_day <- var_mean




#Nuovi positivi
var_mean <- compute_mean_points(df_sub_ext$nuovi_positivi, 3, 3)
df_sub_ext$mean_nuovi_positivi <- var_mean


df_sub <- df_sub_ext[df_sub_ext$data >= inizio_train & df_sub_ext$data <= end_train,]

ggplot(df_sub) +
  geom_line(aes(x=as.Date(data),y=mean_dimessi_guariti_per_day, color="Moving mean")) +
  xlab("Date") + 
  ylab("Dismessi guariti") +
  geom_point(aes(x=as.Date(data),y=dimessi_guariti_per_day))

ggplot(df_sub) +
  geom_line(aes(x=as.Date(data),y=mean_nuovi_positivi, color="Moving mean")) +
  xlab("Date") + 
  ylab("Nuovi positivi") +
  geom_point(aes(x=as.Date(data),y=nuovi_positivi))



```
```{r}

```


This is with respect to `totale_ospedalizzati`, to see linear trend: only `deceduti_per_day` seems to be linear with it!

```{r, echo=FALSE}
ggplot(df_sub, aes(x=totale_ospedalizzati, y=nuovi_positivi, color="nuovi_positivi")) +
  geom_point() + 
  geom_point(data=df_sub,aes(x=totale_ospedalizzati,y=dimessi_guariti_per_day, color="dimessi_guariti_per_day")) +
  geom_point(data=df_sub,aes(x=totale_ospedalizzati,y=deceduti_per_day, color="deceduti_per_day")) +
  xlab("totale_ospedalizzati") + 
  ylab("Variable") + 
  scale_color_manual(name="Variable", values = c(1:3)) +
  facet_zoom(ylim = c(0, 1000))
```

Let's look better:

```{r, echo=FALSE}

data_max_nuovi_positivi=df_sub[which.max(df_sub$nuovi_positivi),]
nuovi_positivi_max_split1 <- df_sub[df_sub$data < data_max_nuovi_positivi$data, ]
nuovi_positivi_max_split2 <- df_sub[df_sub$data >= data_max_nuovi_positivi$data, ]

plot1 <- ggplot() +
  geom_point(aes(x=df_sub$totale_ospedalizzati[df_sub$data < data_max_nuovi_positivi$data], y=nuovi_positivi_max_split1$nuovi_positivi), color="red") + 
  geom_point(aes(x=df_sub$totale_ospedalizzati[df_sub$data >= data_max_nuovi_positivi$data], y=nuovi_positivi_max_split2$nuovi_positivi), color="orange") + 
  xlab("totale_ospedalizzati") + 
  ylab("nuovi_positivi")

plot2 <- ggplot(df_sub, aes(x=totale_ospedalizzati, y=dimessi_guariti_per_day)) +
  geom_point(color="green") + 
  xlab("totale_ospedalizzati") + 
  ylab("dimessi_guariti_per_day")

plot3 <- ggplot(df_sub, aes(x=totale_ospedalizzati, y=deceduti_per_day)) +
  geom_point(color="black") + 
  xlab("totale_ospedalizzati") + 
  ylab("deceduti_per_day")

data_max_nuovi_positivi=df_sub[which.max(df_sub$mean_nuovi_positivi),]
nuovi_positivi_max_split1 <- df_sub[df_sub$data < data_max_nuovi_positivi$data, ]
nuovi_positivi_max_split2 <- df_sub[df_sub$data >= data_max_nuovi_positivi$data, ]

plot4 <- ggplot() +
  geom_point(aes(x=df_sub$totale_ospedalizzati[df_sub$data < data_max_nuovi_positivi$data], y=nuovi_positivi_max_split1$mean_nuovi_positivi), color="red") + 
  geom_point(aes(x=df_sub$totale_ospedalizzati[df_sub$data >= data_max_nuovi_positivi$data], y=nuovi_positivi_max_split2$mean_nuovi_positivi), color="orange") + 
  xlab("totale_ospedalizzati") + 
  ylab("mean_nuovi_positivi")

plot5 <- ggplot(df_sub, aes(x=totale_ospedalizzati, y=mean_dimessi_guariti_per_day)) +
  geom_point(color="green") + 
  xlab("totale_ospedalizzati") + 
  ylab("mean_dimessi_guariti_per_day")
library(ggpubr)
ggarrange(plot1, plot2, plot3, plot4, plot5, ncol = 3, nrow = 3)
```

There are days in which there are more dimessi than 10000 persone which is larger than the total of hospitalized in that day!

## Linear model: forward + occam razor approch 

###  LM1 - 1 covariate

Only the covariate `nuovi_positivi` for regressing the dependent variable `totale_ospedalizzati`: $totale.ospedalizzati=\beta_{0}+\beta_{1}*nuovi.positivi$.

```{r, echo=FALSE}
lm1 <- lm(totale_ospedalizzati ~ nuovi_positivi, data=df_sub)

#See the output 
summary(lm1)
```
```{r, echo=FALSE}
lm1_avg <- lm(totale_ospedalizzati ~ mean_nuovi_positivi, data=df_sub)

#See the output 
summary(lm1_avg)
```

#Adjusted R-squared is better with the mean

Residuals plot:

```{r, echo=FALSE}
##plot of the residuals
par(mfrow=c(2,2))
plot(lm1)
plot(lm1_avg)
```

Predicted points vs actual:

```{r, echo=FALSE}
##plot of the fitted values
par(mfrow=c(1,1))
with(df_sub, plot(nuovi_positivi,totale_ospedalizzati, pch=19))
abline(coef(lm1), col="red", lty="solid")
# or
#curve(predict(lm1, data.frame(dist=x)), col="red", lty="solid", lwd=2, add=TRUE)

text(6100,4400,expression(totale_ospedalizzati==hat(beta)[0]+hat(beta)[1]*dist), col="red")

points(df_sub$nuovi_positivi, predict(lm1), col="red", pch=19, cex=1)
# or
#points(nihills$dist, fitted(lm1), col="red", pch=19, cex=0.8)

segments(df_sub[17,]$nuovi_positivi,df_sub[17,]$totale_ospedalizzati, 
         df_sub[17,]$nuovi_positivi,fitted(lm1)[17], lty="dashed")
```
```{r, echo=FALSE}
##plot of the fitted values
par(mfrow=c(1,1))
with(df_sub, plot(mean_nuovi_positivi,totale_ospedalizzati, pch=19))
abline(coef(lm1_avg), col="red", lty="solid")
# or
#curve(predict(lm1, data.frame(dist=x)), col="red", lty="solid", lwd=2, add=TRUE)

text(6100,4400,expression(totale_ospedalizzati==hat(beta)[0]+hat(beta)[1]*mean_nuovi_positivi), col="red")

points(df_sub$mean_nuovi_positivi, predict(lm1_avg), col="red", pch=19, cex=1)
# or
#points(nihills$dist, fitted(lm1), col="red", pch=19, cex=0.8)

segments(df_sub[17,]$mean_nuovi_positivi,df_sub[17,]$totale_ospedalizzati, 
         df_sub[17,]$mean_nuovi_positivi,fitted(lm1_avg)[17], lty="dashed")
```


### LM1.1 - 1 covariate

Only the covariate `dimessi_guariti_per_day` for regressing the dependent variable `totale_ospedalizzati`: $totale.ospedalizzati=\beta_{0}+\beta_{1}*dimessi.guariti.per.day$.

```{r, echo=FALSE}
lm1.1 <- lm(totale_ospedalizzati ~ dimessi_guariti_per_day, data=df_sub)

#See the output 
summary(lm1.1)
```

```{r, echo=FALSE}
lm1.1_avg <- lm(totale_ospedalizzati ~ mean_dimessi_guariti_per_day, data=df_sub)

#See the output 
summary(lm1.1_avg)
```

#AVG is BETTER that no avg (0.81 >>> 0.34)

**Multiple R-squared (0.3472) is better than the model with the variable `nuovi_positivi` (0.3055), but still residuals are bad.**

Residuals plot:

```{r, echo=FALSE}
##plot of the residuals
par(mfrow=c(2,2))
plot(lm1.1)
plot(lm1.1_avg)
```

Predicted points vs actual:

```{r, echo=FALSE}
##plot of the fitted values
par(mfrow=c(1,1))
with(df_sub, plot(mean_dimessi_guariti_per_day,totale_ospedalizzati, pch=19))
abline(coef(lm1.1_avg), col="red", lty="solid")
# or
#curve(predict(lm1, data.frame(dist=x)), col="red", lty="solid", lwd=2, add=TRUE)

text(6100,4400,expression(totale_ospedalizzati==hat(beta)[0]+hat(beta)[1]*mean_dimessi_guariti_per_day), col="red")

points(df_sub$mean_dimessi_guariti_per_day, predict(lm1.1_avg), col="red", pch=19, cex=1)
# or
#points(nihills$dist, fitted(lm1), col="red", pch=19, cex=0.8)

segments(df_sub[17,]$mean_dimessi_guariti_per_day,df_sub[17,]$totale_ospedalizzati, 
         df_sub[17,]$mean_dimessi_guariti_per_day,fitted(lm1.1_avg)[17], lty="dashed")
```

### LM1.2 - 1 covariate log

Only the covariate `log(dimessi_guariti_per_day)` for regressing the dependent variable `totale_ospedalizzati`: $totale.ospedalizzati=\beta_{0}+\beta_{1}*log(dimessi.guariti.per.day)$.


```{r, echo=FALSE}
lm1.2_avg <- lm(totale_ospedalizzati ~ sqrt(mean_dimessi_guariti_per_day), data=df_sub)

#See the output 
summary(lm1.2_avg)
```

Better with mean (0.86 > 0.62)

with this one with log  (0.867)  is better than previous one (0.81)

**Multiple R-squared (0.6256) is much better than the model with the variable `dimessi_guariti_per_day` without log (0.3472), and residuals are better.**

Residuals plot:

```{r, echo=FALSE}
##plot of the residuals
par(mfrow=c(2,2))
# plot(lm1.2)
plot(lm1.1_avg)
plot(lm1.2_avg)
```

```{r, echo=FALSE}
##plot of the fitted values
par(mfrow=c(1,1))
with(df_sub, plot(mean_dimessi_guariti_per_day,totale_ospedalizzati, pch=19))
# abline(coef(lm1.2_avg), col="red", lty="solid")
# or
curve(predict(lm1.2_avg, data.frame(mean_dimessi_guariti_per_day=x)), col="red", lty="solid", lwd=2, add=TRUE)

text(6100,4400,expression(totale_ospedalizzati==hat(beta)[0]+hat(beta)[1]*mean_dimessi_guariti_per_day), col="red")

points(df_sub$mean_dimessi_guariti_per_day, predict(lm1.2_avg), col="red", pch=19, cex=1)
# or
#points(nihills$dist, fitted(lm1), col="red", pch=19, cex=0.8)

segments(df_sub[17,]$mean_dimessi_guariti_per_day,df_sub[17,]$totale_ospedalizzati, 
         df_sub[17,]$mean_dimessi_guariti_per_day,fitted(lm1.2_avg)[17], lty="dashed")
```

###  LM2 - 2 covariates

Adding also `dimessi_guariti_per_day` for regressing the dependent variable `totale_ospedalizzati`: $totale.ospedalizzati=\beta_{0}+\beta_{1}*nuovi.positivi+\beta_{2}*dimessi.guariti.per.day$.

# ```{r, echo=FALSE}
# lm2 <- lm(totale_ospedalizzati ~ nuovi_positivi + dimessi_guariti_per_day, data=df_sub)
# 
# #See the output 
# summary(lm2)
# ```

```{r, echo=FALSE}
lm2_avg <- lm(totale_ospedalizzati ~ mean_nuovi_positivi + mean_dimessi_guariti_per_day, data=df_sub)

#See the output 
summary(lm2_avg)
```
Better with the mean (0.9 > 0.55)

As you can see, **Adjusted R-squared (0.5557) is better than previous model with just 1 covariate `nuovi_positivi` (0.2998) and 1 covariate `log(dimessi_guariti_per_day)`, but it's worse than model with 1 covariate `log(dimessi_guariti_per_day)` (0.6225) **!

Plot the residuals

```{r, echo=FALSE}
par(mfrow = c(2,2))
plot(lm2_avg)
```

**Now QQ-Plot better understand the high tail, but not the lower one**.
Residuals slightly improved.

###  LM2.1 - 2 covariates, 1 log

Adding also `dimessi_guariti_per_day` for regressing the dependent variable `totale_ospedalizzati`: $totale.ospedalizzati=\beta_{0}+\beta_{1}*nuovi.positivi+\beta_{2}*log(dimessi.guariti.per.day)$.


```{r, echo=FALSE}
lm2.1_avg <- lm(totale_ospedalizzati ~ mean_nuovi_positivi + sqrt(mean_dimessi_guariti_per_day), data=df_sub)

#See the output 
summary(lm2.1_avg)
```

As you can see, **Adjusted R-squared (0.7366) is better than previous model without log (0.5557) and it's even better than model with 1 covariate `log(dimessi_guariti_per_day)` (0.6225) **!

Plot the residuals:

```{r, echo=FALSE}
par(mfrow = c(2,2))
plot(lm2.1_avg)
```

## LM3 - 3 covariates

Adding also `deceduti_per_day` for regressing the dependent variable `totale_ospedalizzati`: $totale.ospedalizzati=\beta_{0}+\beta_{1}*nuovi.positivi+\beta_{2}*dimessi.guariti.per.day+\beta_{3}*deceduti.per.day$.

```{r, echo=FALSE}
lm3_avg <- lm(totale_ospedalizzati ~ mean_nuovi_positivi + mean_dimessi_guariti_per_day + deceduti_per_day, data=df_sub)

#See the output 
summary(lm3_avg)
```

As you can see, **Adjusted R-squared (0.8198) is better than previous model 0.5557**!

Plot the residuals

```{r, echo=FALSE}
par(mfrow = c(2,2))
plot(lm3_avg)
```

**Now QQ-Plot better understand better the tails**.
**Residuals even more slightly improved**.
Is it normal though that if deaths increase, hospitalize people increase? Should be the reverse! It is ok since detch rate is the bottom index that will descrease.

###  LM4 - 3 covariates, 1 with log

Look the histograms of time, dist and climb

```{r, echo=FALSE, fig.width=6, fig.height=6}
par(mfrow=c(2,2))
hist(df_sub$totale_ospedalizzati, probability=TRUE, breaks=20)
hist(df_sub$nuovi_positivi, probability=TRUE, breaks=20)
hist(df_sub$dimessi_guariti_per_day, probability=TRUE, breaks=20)
hist(df_sub$deceduti_per_day, probability=TRUE, breaks=20)
```

```{r, echo=FALSE, fig.width=6, fig.height=6}
par(mfrow=c(2,2))
hist(df_sub$totale_ospedalizzati, probability=TRUE, breaks=20)
hist(df_sub$mean_nuovi_positivi, probability=TRUE, breaks=20)
hist(df_sub$mean_dimessi_guariti_per_day, probability=TRUE, breaks=20)
hist(df_sub$deceduti_per_day, probability=TRUE, breaks=20)
```

The distribution dimessi_guariti_per_day has a long right tail, and lot of values are shrunk towards zero. Furthermore, the extreme point on the right tail -outliers- influences a lot the estimation of the equation line, it has large leverage. Maybe, we need a more symmetric distribution, such as log(yi) above. It is also reasonable to take the logarithm of the explanatory variables.

So let's use the model: $totale.ospedalizzati=\beta_{0}+\beta_{1}*nuovi.positivi+\beta_{2}*log(dimessi.guariti.per.day)+\beta_{3}*deceduti.per.day$.

```{r, echo=FALSE}
lm4_avg <- lm(totale_ospedalizzati ~ mean_nuovi_positivi+ sqrt(mean_dimessi_guariti_per_day) + deceduti_per_day, data=df_sub)
#See the output 
summary(lm4_avg)
print(extractAIC(lm4_avg)[2])
```

```{r, echo=FALSE}
lm4_avg_dummy <- lm(totale_ospedalizzati ~ mean_nuovi_positivi+ sqrt(mean_dimessi_guariti_per_day)*dummy + deceduti_per_day, data=df_sub)
#See the output 
summary(lm4_avg_dummy)
# print(extractAIC(lm4_avg)[2])
```

Now **Adjusted R-squared is 0.8559, better than 0.8198**.

Plot the residuals

```{r, echo=FALSE}
par(mfrow = c(2,2))
plot(lm4_avg)
```
64 is a outliers (molti deceduti, 3 dicembre)

Looking at the residuals plot vs the continuous explanatory variable is a good check to highlight (if any) residual structures in the residuals.

```{r, echo=FALSE}
par(mfrow=c(1,1))
plot(df_sub$dimessi_guariti_per_day, lm4_avg$residuals)
abline(h=0, lty="dashed")
```


###  LM5 - 3 covariates, 2 with log -> EXCLUDE

<!-- Let's use the model: $totale.ospedalizzati=\beta_{0}+\beta_{1}*nuovi.positivi+\beta_{2}*log(dimessi.guariti.per.day)+\beta_{3}*log(deceduti.per.day)$. -->

<!-- ```{r, echo=FALSE} -->
<!-- #Replace first data with 1, since log(0) is error -->
<!-- indexeses <- which(df_sub$deceduti_per_day==0) -->
<!-- df_sub$deceduti_per_day[6] <- 1 -->

<!-- lm5 <- lm(totale_ospedalizzati ~ nuovi_positivi + log(dimessi_guariti_per_day) + log(deceduti_per_day), data=df_sub) -->

<!-- #See the output  -->
<!-- summary(lm5) -->
<!-- ``` -->

<!-- ```{r, echo=FALSE} -->
<!-- par(mfrow = c(2,2)) -->
<!-- plot(lm5) -->
<!-- ``` -->

###  LM6 and 7 - 3 covariates + 1 not interesting

Let's see what happens to Adjusted R-squared when we add one of the 2 not interesting variables:

* `tamponi_per_day`
* `casi_testati_per_day`

First add `tamponi_per_day`.
So let's use the model: $totale.ospedalizzati=\beta_{0}+\beta_{1}*nuovi.positivi+\beta_{2}*log(dimessi.guariti.per.day)+\beta_{3}*deceduti.per.day+\beta_{4}tamponi_per_day$.

```{r, echo=FALSE}
# Fit the linear model 1: time = beta_0+beta_1*nuovi_positivi + epsilon
lm6_avg <- lm(totale_ospedalizzati ~ mean_nuovi_positivi+ sqrt(mean_dimessi_guariti_per_day) + deceduti_per_day + tamponi_per_day, data=df_sub)

#See the output 
summary(lm6_avg)
```

**Firstly, the t-test of `tamponi_per_day` leads to accept the null hypotesis: coeff is equal to 0**.
**Then the Adjusted R-squared is better 0.8626, slightly more than before (0.8559)**.-< how cares! t-test sucks!

Now add `casi_testati_per_day`.
So let's use the model: $totale.ospedalizzati=\beta_{0}+\beta_{1}*nuovi.positivi+\beta_{2}*log(dimessi.guariti.per.day)+\beta_{3}*deceduti.per.day+\beta_{4}casi_testati_per_day$.

```{r, echo=FALSE}
lm7_avg <- lm(totale_ospedalizzati ~ mean_nuovi_positivi+ sqrt(mean_dimessi_guariti_per_day) + deceduti_per_day + casi_testati_per_day, data=df_sub)

#See the output 
summary(lm7_avg)
```
Non lo teniamo in considerazione perch√© ha lo stesso R^2 ma √® pi√π complicato il modello (ce lo aspettevamo perch√© logicamente non √® significatico)


**Firstly, the t-test of `casi_testati_per_day` is good with \alpha, but the Intercept t-test is worse than the normal log model**.
**But the Adjusted R-squared has increased 0.8852, better than before (0.8559)**.



What should we do? Take it into account? I think not since `casi_testati_per_day` is the number of total persons tested either positive or negative to Covid19, so maybe in those days the number person being tested were more everyday since there was more disposal of covid tests!

## LM models confront

Let's see if there is multicollinearity between covariates in all the LM models:

```{r, echo=FALSE}
print("lm2")
vif(lm2_avg)
print("lm2.1")
vif(lm2.1_avg)
print("lm3")
vif(lm3_avg)
print("lm4")
vif(lm4_avg)

print("lm6")
vif(lm6_avg)
print("lm7")
vif(lm7_avg)
#See the output 
```
Since all indexes are lower than 10, there is no multicollinearity between covariates.

Let's check the AIC values:

```{r, echo=FALSE}
# AIC
AIC <- rbind(extractAIC(lm1_avg)[2],extractAIC(lm1.2_avg)[2],
             extractAIC(lm2_avg)[2],extractAIC(lm2.1_avg)[2],
             extractAIC(lm3_avg)[2],extractAIC(lm4_avg)[2],
             extractAIC(lm6_avg)[2],
             extractAIC(lm7_avg)[2]
             )
#See the output 
AIC
```
I had the AIC of the three models with only 1 variable and the first model with 2 variables:

<!-- # ```{r echo=TRUE} -->
<!-- # # AIC -->
<!-- # AIC2 <- rbind(extractAIC(lm1)[2],extractAIC(lm1.1)[2], -->
<!-- #              extractAIC(lm1.2)[2],extractAIC(lm2)[2], -->
<!-- #              extractAIC(lm2.1)[2] -->
<!-- #              ) -->
<!-- # #See the output  -->
<!-- # AIC2 -->
<!-- # ``` -->

Has expected by R^2 Adj it is better the model with one covariate `log(dimessi_guariti_per_day)` than the model with 2 covariates in which none is log, but the best is the one with 2 covariets in which ther is log.

**LM4 model has the second minimum AIC**, if we exclude LM6 (since R2 is the worse). 
LM7 has the best score but the variable is meaningless to our purpose.

### LM comparison (F-test)

We perform an anova test to check two nested models. We compare lm1.2 and lm2.1 since thay are the best between all the models with one coefficient and all the models with two coefficients:

```{r}
anova(lm1.2_avg, lm2.1_avg, test='F')
```
lm2.1 is preferred to lm1.2 as expected. Since lm3 and lm4 have the same number of covariates and lm4 is better than lm3, we compare lm2.1 with lm4:
```{r}
anova(lm2.1_avg, lm4_avg, test='F')
```
lm4 is preferred to lm2.1. We do not compare lm4 with lm6 since the latter as the coeeficent of tamp_per_day equal to zero.
<!-- We compare lm4 with lm7: -->
# ```{r}
# anova(lm4_avg, lm7_avg, test='F')
# ```
<!-- lm7 is significant, but we do not use it. -->

## GLM con Poisson: reverse approch

Now we can relax the hypotesis of normality, using a distribution with support in $[0,\inf]$.

###  GLM1 - 3 covariates, 1 log

```{r, echo=FALSE}
glm1 <- glm(totale_ospedalizzati ~ nuovi_positivi + log(dimessi_guariti_per_day) + deceduti_per_day, data=df_sub, poisson)
summary(glm1)
```

```{r, echo=FALSE}
glm1_avg <- glm(totale_ospedalizzati ~ mean_nuovi_positivi + log(mean_dimessi_guariti_per_day) + deceduti_per_day, data=df_sub, poisson)
summary(glm1_avg)
```


With dummy
```{r, echo=FALSE}
glm1_avg_dummy <- glm(totale_ospedalizzati ~ mean_nuovi_positivi + sqrt(mean_dimessi_guariti_per_day)*dummy + deceduti_per_day, data=df_sub, poisson)
summary(glm1_avg_dummy)
```

Null deviance: 207553  on 122  degrees of freedom
Residual deviance:  36112  on 119  degrees of freedom
**Residual D = 36112**
**AIC: 37352**

lm7 is better for AIC (1488)

###  GLM2 - 3 covariates

```{r, echo=FALSE}
glm2_avg <- glm(totale_ospedalizzati ~ mean_nuovi_positivi + mean_dimessi_guariti_per_day + deceduti_per_day, data=df_sub, poisson)
summary(glm2_avg)
```

<!-- Null deviance: 207553  on 122  degrees of freedom. -->
<!-- Residual deviance:  73889  on 119  degrees of freedom. -->

<!-- **Residual D = 73889, than GLM1 is better**. -->
<!-- **AIC: 75130**. -->

###  GLM3 - 2 covariates, 1 log

```{r, echo=FALSE}
glm3_avg <- glm(totale_ospedalizzati ~ mean_nuovi_positivi + sqrt(mean_dimessi_guariti_per_day), data=df_sub, poisson)
summary(glm3_avg)
```

AIC: 38357
GLM1 --> AIC: 37352

###  GLM4 - 2 covariates

```{r, echo=FALSE}
glm4_avg <- glm(totale_ospedalizzati ~ mean_nuovi_positivi + mean_dimessi_guariti_per_day, data=df_sub, poisson)
summary(glm4_avg)
```

###  GLM5 - 1 covariate

```{r, echo=FALSE}
glm5_avg <- glm(totale_ospedalizzati ~ mean_nuovi_positivi, data=df_sub, poisson)
summary(glm5_avg)
```


We can use the residual deviance to perform a goodness of fit test for the overall model. The residual deviance is the difference between the deviance of the current model and the maximum deviance of the ideal model where the predicted values are identical to the observed. Therefore, if the residual difference is small enough, the goodness of fit test will not be significant, indicating that the model fits the data. We conclude that the model fits reasonably well because the goodness-of-fit chi-squared test is not statistically significant. If the test had been statistically significant, it would indicate that the data do not fit the model well. In that situation, we may try to determine if there are omitted predictor variables, if our linearity assumption holds and/or if there is an issue of over-dispersion.

### GLM comparison (Chisq-test)
we compare glm3 and glm1 sicne they  are two nested models. glm2 has the same number of coefficents that glm1 but is worst, so it does not make ense to compare glm2 with glm3.

```{r}
anova(glm3_avg, glm1_avg, test='Chisq')
```
The p-value of glm1 is significant, thus we reject H0. glm1 is better than glm3.

Now we compare glm1 and glm5:
```{r}
anova(glm5_avg, glm1_avg, test='Chisq')
```
glm1 remains statisticalliy significant. We can conclude that it is the best among all the glm models.

## Little thoery digression

Generalized linear model is defined in terms of linear predictor

ùúÇ=ùëãùõΩ
that is passed through the link function ùëî:

ùëî(ùê∏(ùëå|ùëã))=ùúÇ
It models the relation between the dependent variable ùëå and independent variables ùëã=ùëã1,ùëã2,‚Ä¶,ùëãùëò. More precisely, it models a conditional expectation of ùëå given ùëã,

ùê∏(ùëå|ùëã)=ùúá=ùëî‚àí1(ùúÇ)
so the model can be defined in probabilistic terms as

ùëå|ùëã‚àºùëì(ùúá,ùúé2)
where ùëì is a probability distribution of the exponential family. So first thing to notice is that ùëì is not the distribution of ùëå, but ùëå follows it conditionally on ùëã. The choice of this distribution depends on your knowledge (what you can assume) about the relation between ùëå and ùëã. So anywhere you read about the distribution, what is meant is the conditional distribution.

If your outcome is continuous and unbounded, then the most "default" choice is the Gaussian distribution (a.k.a. normal distribution), i.e. the standard linear regression (unless you use other link function then the default identity link).

If you are dealing with continuous non-negative outcome, then you could consider the Gamma distribution, or Inverse Gaussian distribution.

If your outcome is discrete, or more precisely, you are dealing with counts (how many times something happen in given time interval), then the most common choice of the distribution to start with is Poisson distribution. The problem with Poisson distribution is that it is rather inflexible in the fact that it assumes that mean is equal to variance, if this assumption is not met, you may consider using quasi-Poisson family, or negative binomial distribution (see also Definition of dispersion parameter for quasipoisson family).

If your outcome is binary (zeros and ones), proportions of "successes" and "failures" (values between 0 and 1), or their counts, you can use Binomial distribution, i.e. the logistic regression model. If there is more then two categories, you would use multinomial distribution in multinomial regression.

On another hand, in practice, if you are interested in building a predictive model, you may be interested in testing few different distributions, and in the end learn that one of them gives you more accurate results then the others even if it is not the most "appropriate" in terms of theoretical considerations (e.g. in theory you should use Poisson, but in practice standard linear regression works best for your data).

```{r eval=FALSE, include=FALSE}
#par(mfrow=c(2,2))
n <- length(df_sub$totale_ospedalizzati) # n observations
hist(df_sub$totale_ospedalizzati, probability=TRUE, breaks=30)
mean <- mean(df_sub$totale_ospedalizzati)
x <- seq(0,9340)
#curve(dnorm(x, mean, 1), col = "red", lwd = 2, add = TRUE)}
lines(x, dnorm(x, mean, n), col = "red", lwd = 2)
lines(x, dchisq(x, df = n), col = "green", lwd = 1.5)
lines(x, dgamma(x, shape = 5000, rate = 1), col = "blue", lwd = 1.5)
lines(x, dcauchy(x, location = mean, scale = 1000, log = FALSE), col = "pink", lwd = 2)
lines(x, dpois(x, mean, log = FALSE), col = "black", lwd = 1.5)
```

Let's look for overdispertion (in Poisson $\phi=1$):

```{r, echo=FALSE}
check_overdispersion(glm1_avg)
check_overdispersion(glm1)
```

There is, so let's try another distribution.

## QuasiPoisson GLMs

In quasi-likelihood method we relax the assumption of having an exponential family distribution.
**The parameters will be the same of GLM but their std.error will be different.**

### QP-GLM with 3 covariates

```{r, echo=FALSE}
qp.glm1 <- glm(totale_ospedalizzati ~ nuovi_positivi+ dimessi_guariti_per_day + deceduti_per_day, data=df_sub, quasipoisson)
summary(qp.glm1)
print("Real Residual Deviance:")
print(sum(residuals.glm(qp.glm1, "deviance")^2)/summary(qp.glm1)$dispersion)
```
Residuals don't follow a normal distribution.
Null deviance: 207553  on 122  degrees of freedom
**Residual D = 140.4591**  -> divided by dispertion
**AIC: NA**

```{r, echo=FALSE}
qp.glm1_avg <- glm(totale_ospedalizzati ~ mean_nuovi_positivi+ mean_dimessi_guariti_per_day + deceduti_per_day, data=df_sub, quasipoisson)
summary(qp.glm1_avg)
print("Real Residual Deviance:")
print(sum(residuals.glm(qp.glm1_avg, "deviance")^2)/summary(qp.glm1_avg)$dispersion)
```

### QP-GLM with 3 covariates, 1 log

```{r, echo=FALSE}
qp.glm2 <- glm(totale_ospedalizzati ~ nuovi_positivi+ log(dimessi_guariti_per_day) + deceduti_per_day, data=df_sub, quasipoisson)
summary(qp.glm2)

print("Real Residual Deviance:")
print(sum(residuals.glm(qp.glm2, "deviance")^2)/summary(qp.glm2)$dispersion)

```

Residuals seems to follow a normal distribution.
Null deviance: 207553  on 122  degrees of freedom
**Residual D = 128.2974**  -> better then GLM1
**AIC: NA**

Remember the same model but with GLM:

glm(formula = totale_ospedalizzati ~ nuovi_positivi + log(dimessi_guariti_per_day) + 
    deceduti_per_day, family = poisson, data = df_sub)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-45.273  -11.845   -0.844   10.692   59.762  

Coefficients:
                              Estimate Std. Error z value Pr(>|z|)    
(Intercept)                  6.018e+00  1.038e-02  579.75   <2e-16 ***
nuovi_positivi               4.154e-05  4.889e-07   84.97   <2e-16 ***
log(dimessi_guariti_per_day) 2.553e-01  1.442e-03  177.04   <2e-16 ***
deceduti_per_day             3.300e-03  2.483e-05  132.91   <2e-16 ***


```{r, echo=FALSE}
qp.glm2_avg <- glm(totale_ospedalizzati ~ mean_nuovi_positivi+ log(mean_dimessi_guariti_per_day) + deceduti_per_day, data=df_sub, quasipoisson)
summary(qp.glm2_avg)

print("Real Residual Deviance:")
print(sum(    residuals.glm(qp.glm2_avg, "deviance")^2)/summary(qp.glm2_avg)$dispersion)


```

La dispersione con qp.glm2_avg √® minore e il real residual deviance viene maggiore


### Negative Binomial GLM (not a member of GLM), with old

Better not to use cause there is another varibale added and plus they might asks questions.

```{r, echo=FALSE}
glm8 <- glm.nb(totale_ospedalizzati ~ nuovi_positivi+ log(dimessi_guariti_per_day) + deceduti_per_day, data=df_sub)
summary(glm8)

```

### Negative Binomial GLM (not a member of GLM), with average

```{r, echo=FALSE}
glm8_avg <- glm.nb(totale_ospedalizzati ~ mean_nuovi_positivi+ sqrt(mean_dimessi_guariti_per_day) + deceduti_per_day, data=df_sub)
summary(glm8_avg)

```
### Use of Gamma count?

Event counts are response variables with non-negative integer values representing the number of times that an event occurs within a fixed domain such as a time interval, a geographical area or a cell of a contingency table. Analysis of counts by Gaussian regression models ignores the discreteness, asymmetry and heterocedasticity and is inefficient, providing unrealistic standard errors or possibily negative predictions of the expected number of events. The Poisson regression is the standard model for count data with underlying assumptions on the generating process which may be implausible in many applications. Statisticians have long recognized the limitation of imposing equidispersion under the Poisson regression model. A typical situation is when the conditional variance exceeds the conditional mean, in which case models allowing for overdispersion are routinely used. One of such alternatives, the Gamma-count model, is adopted here and results show improvements over the Poisson model (for the semiparametric quasi-Poisson model we should check with another criteria -not AIC-) in capturing the observed variability in the data.


### GAMMA-GLM with 3 covariates, 1 log, old, inverse link

```{r, echo=FALSE}
gamma.glm1 <- glm(totale_ospedalizzati ~ nuovi_positivi+ log(dimessi_guariti_per_day) + deceduti_per_day, data=df_sub, family = Gamma(link = "inverse"))

summary(gamma.glm1)
print("Test usefullness:")
print(1 - pchisq(68.416 -37.058, 3))#2.344397e-05
print("Model goodness:")
print(with(summary(gamma.glm1), 1 - deviance/null.deviance))

```

Residuals seems to follow a normal distribution but very thin!
Null deviance: 207553  on 122  degrees of freedom
**Residual D = 37.058  -> even better than QS Poisson but it's lower than DF: is it correct?**
**AIC: 2229.9 -> NiB is better (2174.7), but this is better than GLM1 (54942)**

### GAMMA-GLM with 3 covariates, 1 sqrt, average, inverse link

```{r, echo=FALSE}
gamma.glm1_avg <- glm(totale_ospedalizzati ~ mean_nuovi_positivi+ sqrt(mean_dimessi_guariti_per_day) + deceduti_per_day, data=df_sub, family = Gamma(link = "inverse"))

summary(gamma.glm1_avg)
print("Test usefullness:")
print(1 - pchisq(68.416 -33.981, 3))#2.080025e-09
print("Model goodness:")
print(with(summary(gamma.glm1_avg), 1 - deviance/null.deviance))
#AIC 2218.7
```

### GAMMA-GLM with 3 covariates, 1 log, old, log link

```{r, echo=FALSE}
gamma.glm2 <- glm(totale_ospedalizzati ~ nuovi_positivi+ log(dimessi_guariti_per_day) + deceduti_per_day, data=df_sub, family = Gamma(link = "log"))

summary(gamma.glm2)
print("Test usefullness:")
print(1 - pchisq(68.416 -24.132, 3))#2.344397e-05
print("Model goodness:")
print(with(summary(gamma.glm2), 1 - deviance/null.deviance))

```


Residuals seems to follow a normal distribution.
Null deviance: 207553  on 122  degrees of freedom
**Residual D = 24.132  -> even better than Gamma with inverse link func, but it's even lower than DF: is it still correct?**
**AIC: 2175 -> is very similar to the lowest NiB (2174.7)**

### GAMMA-GLM with 3 covariates, 1 sqrt, average, log link

```{r, echo=FALSE}
gamma.glm2_avg <- glm(totale_ospedalizzati ~ mean_nuovi_positivi+ log(mean_dimessi_guariti_per_day), data=df_sub, family = Gamma(link = "log"))

summary(gamma.glm2_avg)
print("Test usefullness:")
print(1 - pchisq(68.416 -19.979, 3))#1.313438e-09
print("Model goodness:")
print(with(summary(gamma.glm2_avg), 1 - deviance/null.deviance))

```

## GAM

```{r, echo=FALSE}
gam1 <- gam(totale_ospedalizzati ~ s(nuovi_positivi) + log(dimessi_guariti_per_day) + deceduti_per_day, data=df_sub)
summary(gam1)
#save.image(file='data_V1.RData')
```
```{r, echo=FALSE}
gam1_avg <- gam(totale_ospedalizzati ~ s(mean_nuovi_positivi) + sqrt(mean_dimessi_guariti_per_day) + deceduti_per_day, data=df_sub)
summary(gam1_avg)
#save.image(file='data_V1.RData')
```
```{r, echo=FALSE}
gam2 <- gam(totale_ospedalizzati ~ s(nuovi_positivi) + log(dimessi_guariti_per_day) + deceduti_per_day, data=df_sub, family = poisson(link = log))
summary(gam2)
#save.image(file='data_V2.RData')
```



```{r, echo=FALSE}
gam2_avg <- gam(totale_ospedalizzati ~ s(mean_nuovi_positivi) + log(mean_dimessi_guariti_per_day) + deceduti_per_day, data=df_sub, family = poisson(link = log))
summary(gam2_avg)
#save.image(file='data_V1.RData')
```

```{r echo=TRUE}

#FARE AVG SE SERVE
anova(gam1, gam2, test="Chisq")
anova(gam1_avg, gam2_avg, test="Chisq")

# AIC(gam1)
AIC(gam2)
# AIC(gam1_avg)
AIC(gam2_avg)

# summary(gam1)$sp.criterion
summary(gam2)$sp.criterion
summary(gam2_avg)$sp.criterion
#small bias and less overfitting for smaller

# summary(gam1)$r.sq
summary(gam2)$r.sq  # adjusted R squared
summary(gam2_avg)$r.sq 
#save.image(file='data_V1.RData')
```
```{r, echo=FALSE}
# Look the coefficient of gam2
coef(gam2)
# save.image(file='data_V2.RData')
```

Diagnostic plot involve the representation of the smooth function and the partial residuals defined as:
œµÃÇ partij=≈ù j(xij)+œµÃÇ Pi
where œµÃÇ P are the Pearson residuals of the model. Looking at this plot we are interested in noticing non linearity or wiggle behavior of the smooth function and if the partial residuals are evenly distributed around the function.

```{r, echo=FALSE}
# Diagnostic plot:
plot(gam2, residuals = TRUE, pch = 19, pages = 1)
```
## Adding Regional Colors as covariate - qp.glm2_avg
```{r, echo=FALSE}
qp.glm2_avg.colors <- glm(totale_ospedalizzati ~ mean_nuovi_positivi+ log(mean_dimessi_guariti_per_day) + deceduti_per_day + color, data=df_sub, quasipoisson)
summary(qp.glm2_avg.colors)
```
```{r, echo=FALSE}
print("Real Residual Deviance qp.glm2_avg:")
print(sum(    residuals.glm(qp.glm2_avg, "deviance")^2)/summary(qp.glm2_avg)$dispersion)

print("Real Residual Deviance qp.glm2_avg.colors:")
print(sum(    residuals.glm(qp.glm2_avg.colors, "deviance")^2)/summary(qp.glm2_avg.colors)$dispersion)
``` 

## Adding Regional Colors delayed - qp.glm2_avg
```{r, echo=FALSE}
qp.glm2_avg.colors_del <- glm(totale_ospedalizzati ~ mean_nuovi_positivi+ log(mean_dimessi_guariti_per_day) + deceduti_per_day + color_delayed, data=df_sub, quasipoisson)
summary(qp.glm2_avg.colors_del)
``` 
```{r, echo=FALSE}
print("Real Residual Deviance qp.glm2_avg:")
print(sum(    residuals.glm(qp.glm2_avg, "deviance")^2)/summary(qp.glm2_avg)$dispersion)

print("Real Residual Deviance qp.glm2_avg.colors_del:")
print(sum(residuals.glm(qp.glm2_avg.colors_del, "deviance")^2)/summary(qp.glm2_avg.colors_del)$dispersion)
``` 
## Adding Regional Colors as covariate - glm1_avg
```{r, echo=FALSE}
glm1_avg.colors <- glm(totale_ospedalizzati ~ mean_nuovi_positivi + log(mean_dimessi_guariti_per_day) + deceduti_per_day + color, data=df_sub, poisson)
summary(glm1_avg.colors)
```
```{r, echo=FALSE}
AIC(glm1_avg, glm1_avg.colors)
``` 
## Adding Regional Colors delayed - glm1_avg
```{r, echo=FALSE}
glm1_avg.colors_del <- glm(totale_ospedalizzati ~ mean_nuovi_positivi + log(mean_dimessi_guariti_per_day) + deceduti_per_day + color_delayed, data=df_sub, poisson)
summary(glm1_avg.colors_del)
```
```{r, echo=FALSE}
AIC(glm1_avg, glm1_avg.colors_del)
``` 
## Adding Regional Colors as covariate - gamma.glm2_avg
```{r, echo=FALSE}
gamma.glm2_avg.colors <- glm(totale_ospedalizzati ~ mean_nuovi_positivi+ log(mean_dimessi_guariti_per_day)+color, data=df_sub, family = Gamma(link = "log"))
summary(gamma.glm2_avg.colors)
```
```{r, echo=FALSE}
AIC(gamma.glm2_avg, gamma.glm2_avg.colors)
```
## Adding Regional Colors delayed - gamma.glm2_avg
```{r, echo=FALSE}
gamma.glm2_avg.colors_del <- glm(totale_ospedalizzati ~ mean_nuovi_positivi+ log(mean_dimessi_guariti_per_day) + color_delayed, data=df_sub, family = Gamma(link = "log"))
summary(gamma.glm2_avg.colors_del)
```
```{r, echo=FALSE}
AIC(gamma.glm2_avg, gamma.glm2_avg.colors_del)
```
## Adding Regional Colors as covariate - gam2_avg
```{r, echo=FALSE}
gam2_avg.colors <- gam(totale_ospedalizzati ~ s(mean_nuovi_positivi) + log(mean_dimessi_guariti_per_day) + deceduti_per_day + color, data=df_sub, family = poisson(link = log))
summary(gam2_avg.colors)
```
```{r, echo=FALSE}
AIC(gam2_avg, gam2_avg.colors)
```
## Adding Regional Colors delayed  - gam2_avg
```{r, echo=FALSE}
gam2_avg.colors_del <- gam(totale_ospedalizzati ~ s(mean_nuovi_positivi) + log(mean_dimessi_guariti_per_day) + deceduti_per_day + color_delayed, data=df_sub, family = poisson(link = log))
summary(gam2_avg.colors_del)
```
```{r, echo=FALSE}
AIC(gam2_avg, gam2_avg.colors_del)
```
## Adding Regional RT  - qp.glm2_avg
```{r, echo=FALSE}
qp.glm2_avg.rt<- glm(totale_ospedalizzati ~ mean_nuovi_positivi+ log(mean_dimessi_guariti_per_day) + deceduti_per_day + rt, data=df_sub, quasipoisson)
summary(qp.glm2_avg.rt)
```

```{r}
qp.glm2_avg.rt
```

```{r, echo=FALSE}
print("Real Residual Deviance qp.glm2_avg:")
print(sum(    residuals.glm(qp.glm2_avg, "deviance")^2)/summary(qp.glm2_avg)$dispersion)

print("Real Residual Deviance qp.glm2_avg.rt:")
print(sum(    residuals.glm(qp.glm2_avg.rt, "deviance")^2)/summary(qp.glm2_avg.rt)$dispersion)
```
## Adding Regional RT  - glm1_avg 
```{r, echo=FALSE}
glm1_avg.rt <- glm(totale_ospedalizzati ~ mean_nuovi_positivi + log(mean_dimessi_guariti_per_day) + deceduti_per_day + rt, data=df_sub, poisson)
summary(glm1_avg.rt)
```
```{r, echo=FALSE}
AIC(glm1_avg, glm1_avg.rt)
``` 
## Adding Regional RT  - gamma.glm2_avg 
```{r, echo=FALSE}
gamma.glm2_avg.rt <- glm(totale_ospedalizzati ~ mean_nuovi_positivi+ log(mean_dimessi_guariti_per_day) + rt, data=df_sub, family = Gamma(link = "log"))
summary(gamma.glm2_avg.rt)
```
```{r, echo=FALSE}
AIC(gamma.glm2_avg, gamma.glm2_avg.rt)
``` 
## Adding Regional RT  - gam2_avg
```{r, echo=FALSE}
gam2_avg.rt <- gam(totale_ospedalizzati ~ s(mean_nuovi_positivi) + log(mean_dimessi_guariti_per_day) + deceduti_per_day + rt, data=df_sub, family = poisson(link = log))
summary(gam2_avg.rt)
```
```{r, echo=FALSE}
AIC(gam2_avg, gam2_avg.rt)
``` 











## PREDICTION

```{r}
df_pred <- df_sub_ext[df_sub_ext$data >= "2021-02-02T00:00:00" & df_sub_ext$data <= "2021-02-16T00:00:00",]

```

### LM

```{r, echo=FALSE}
lm4_pred<-predict(lm4_avg, newdata = df_pred, interval = "prediction", level=0.95)


df_pred$lm4_pred<-as.numeric(lm4_pred[,'fit'])

mse_lm4<-mean((df_pred$totale_ospedalizzati - df_pred$lm4_pred)^2)

```

```{r, echo=FALSE}
df_grafico<-df_sub
df_grafico$lm4_pred<-df_sub$totale_ospedalizzati
df_grafico<-rbind(df_grafico, df_pred)
```
```{r}
ggplot(df_grafico, aes(x=as.Date(data), y=totale_ospedalizzati, col='red')) +
  geom_line() +
  xlab("Date") +
  scale_y_continuous(breaks = seq(0,9500, by=1000))+
  geom_line(aes(x=data, y=lm4_pred, col='black'))
```


### LM DUMMY

```{r, echo=FALSE}
lm4_pred_dummy<-predict(lm4_avg_dummy, newdata = df_pred, interval = "prediction", level=0.95)


df_pred$lm4_pred_dummy<-as.numeric(lm4_pred_dummy[,'fit'])

mse_lm4_dummy<-mean((df_pred$totale_ospedalizzati - df_pred$lm4_pred_dummy)^2)

```

```{r, echo=FALSE}
df_grafico<-df_sub
df_grafico$lm4_pred_dummy<-df_sub$totale_ospedalizzati
df_grafico<-rbind(df_grafico, df_pred)
```
```{r}
ggplot(df_grafico, aes(x=as.Date(data), y=totale_ospedalizzati, col='red')) +
  xlab("Date") +
  scale_y_continuous(breaks = seq(0,9500, by=1000))+
  geom_line(aes(x=data, y=lm4_pred_dummy, col='black')) +
    geom_line() 
```

### GLM
```{r, echo=FALSE}
glm1_avg_pred<-predict(glm1_avg, newdata = df_pred, type = "response", interval = "prediction", level=0.95)

mse_glm1<-mean((df_pred$totale_ospedalizzati - as.numeric(glm1_avg_pred))^2)

df_pred$glm1_avg_pred<-as.numeric(glm1_avg_pred)

```

```{r, echo=FALSE}
df_grafico<-df_sub
df_grafico$lm4_pred<-df_sub$totale_ospedalizzati
df_grafico$glm1_avg_pred<-df_sub$totale_ospedalizzati
df_grafico$glm1_avg_dummy_pred<-df_sub$totale_ospedalizzati
df_grafico<-rbind(df_grafico, df_pred)

ggplot(df_grafico, aes(x=as.Date(data), y=totale_ospedalizzati, col='red') ) +
  geom_line() +
  xlab("Date") +
  scale_y_continuous(breaks = seq(0,9500,by=1000))+
  geom_line(aes(x=as.Date(data), y=glm1_avg_pred, col='black')) 
    # geom_errorbar(aes(ymin = lower, ymax = upper))
               

```


### GLM DUMMY
```{r, echo=FALSE}
glm1_avg_dummy_pred<-predict(glm1_avg_dummy, newdata = df_pred, type = "response", interval = "prediction", level=0.95)

mse_glm1_dummy<-mean((df_pred$totale_ospedalizzati - as.numeric(glm1_avg_dummy_pred))^2)

df_pred$glm1_avg_dummy_pred<-as.numeric(glm1_avg_dummy_pred)

```

```{r, echo=FALSE}
df_grafico<-df_sub
df_grafico$lm4_pred<-df_sub$totale_ospedalizzati
df_grafico$glm1_avg_pred<-df_sub$totale_ospedalizzati
df_grafico$glm1_avg_dummy_pred<-df_sub$totale_ospedalizzati
df_grafico<-rbind(df_grafico, df_pred)

ggplot(df_grafico, aes(x=as.Date(data), y=totale_ospedalizzati, col='totale_ospedalizzati') ) +
  
  xlab("Date") +
  scale_y_continuous(breaks = seq(0,9500,by=1000))+
  geom_line(aes(x=as.Date(data), y=glm1_avg_dummy_pred, col="glm1_avg_dummy_pred") ) +
    geom_line() 
    # geom_errorbar(aes(ymin = lower, ymax = upper))
               

```



### GLM_QP
```{r}
qp.glm2_avg_pred<-predict(qp.glm2_avg, newdata = df_pred, type = "response")

mse_qp.glm2<-mean((df_pred$totale_ospedalizzati - as.numeric(qp.glm2_avg_pred))^2)

df_pred$qp.glm2_avg_pred<-as.numeric(qp.glm2_avg_pred)

```

```{r, echo=FALSE}
df_grafico<-df_sub
df_grafico$lm4_pred<-df_sub$totale_ospedalizzati
df_grafico$glm1_avg_pred<-df_sub$totale_ospedalizzati
df_grafico$qp.glm2_avg_pred<-df_sub$totale_ospedalizzati
df_grafico<-rbind(df_grafico, df_pred)

ggplot(df_grafico, aes(x=as.Date(data), y=totale_ospedalizzati, col='red') ) +
  geom_line() +
  xlab("Date") +
  scale_y_continuous(breaks = seq(0,9500,by=1000))+
  geom_line(aes(x=as.Date(data), y=qp.glm2_avg_pred, col='black')) 
    # geom_errorbar(aes(ymin = lower, ymax = upper))
               

```

### Gamma GLM
```{r}
gamma.glm2_avg_pred<-predict(gamma.glm2_avg, newdata = df_pred, type = "response")

mse_gamma.glm2<-mean((df_pred$totale_ospedalizzati - as.numeric(gamma.glm2_avg_pred))^2)

df_pred$gamma.glm2_avg_pred<-as.numeric(gamma.glm2_avg_pred)

```

```{r, echo=FALSE}
df_grafico<-df_sub
df_grafico$lm4_pred<-df_sub$totale_ospedalizzati
df_grafico$glm1_avg_pred<-df_sub$totale_ospedalizzati
df_grafico$qp.glm2_avg_pred<-df_sub$totale_ospedalizzati
df_grafico$gamma.glm2_avg_pred<-df_sub$totale_ospedalizzati
df_grafico<-rbind(df_grafico, df_pred)

ggplot(df_grafico, aes(x=as.Date(data), y=totale_ospedalizzati, col='red') ) +
  geom_line() +
  xlab("Date") +
  scale_y_continuous(breaks = seq(0,9500,by=1000))+
  geom_line(aes(x=as.Date(data), y=gamma.glm2_avg_pred, col='black')) 
    # geom_errorbar(aes(ymin = lower, ymax = upper))
               

```


```{r}
gammma.gam1_avg <- gam(totale_ospedalizzati ~ s(mean_nuovi_positivi) + log(mean_dimessi_guariti_per_day) + deceduti_per_day, data=df_sub, family = Gamma(link = "log"))
summary(gammma.gam1_avg)
print("AIC")
print(AIC(gammma.gam1_avg))
```
```{r}
gammma.gam1_avg_pred<-predict(gammma.gam1_avg, newdata = df_pred, type = "response")


df_pred$gammma.gam1_avg_pred<-as.numeric(gammma.gam1_avg_pred)
```

```{r, echo=FALSE}
df_grafico<-df_sub
df_grafico$lm4_pred<-df_sub$totale_ospedalizzati
df_grafico$glm1_avg_pred<-df_sub$totale_ospedalizzati
df_grafico$qp.glm2_avg_pred<-df_sub$totale_ospedalizzati
df_grafico$gamma.glm2_avg_pred<-df_sub$totale_ospedalizzati
df_grafico$gammma.gam1_avg_pred<-df_sub$totale_ospedalizzati
df_grafico<-rbind(df_grafico, df_pred)

ggplot(df_grafico, aes(x=as.Date(data), y=totale_ospedalizzati, col='red') ) +
  geom_line() +
  xlab("Date") +
  scale_y_continuous(breaks = seq(0,9500,by=1000))+
  geom_line(aes(x=as.Date(data), y=gammma.gam1_avg_pred, col='black')) 
    # geom_errorbar(aes(ymin = lower, ymax = upper))
               

```
```{r}

qp.glm2_avg.rt_pred<-predict(qp.glm2_avg.rt, newdata = df_pred, type = "response")

df_pred$qp.glm2_avg.rt_pred<-as.numeric(qp.glm2_avg.rt_pred)

```
```{r}
df_grafico<-df_sub
df_grafico$lm4_pred<-df_sub$totale_ospedalizzati
df_grafico$glm1_avg_pred<-df_sub$totale_ospedalizzati
df_grafico$qp.glm2_avg_pred<-df_sub$totale_ospedalizzati
df_grafico$gamma.glm2_avg_pred<-df_sub$totale_ospedalizzati
df_grafico$gammma.gam1_avg_pred<-df_sub$totale_ospedalizzati
df_grafico$qp.glm2_avg.rt_pred<-df_sub$totale_ospedalizzati
df_grafico<-rbind(df_grafico, df_pred)

ggplot(df_grafico, aes(x=as.Date(data), y=totale_ospedalizzati, col='red') ) +
  geom_line() +
  xlab("Date") +
  scale_y_continuous(breaks = seq(0,9500,by=1000))+
  geom_line(aes(x=as.Date(data), y=gammma.gam1_avg_pred, col='black')) 
    # geom_errorbar(aes(ymin = lower, ymax = upper))
               
```

